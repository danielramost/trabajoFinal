{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se llama a una función que habilita las funcionalidades de Spark en esta sesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se cargan los datos descargados de la página indicada por el tutorial que se encuentran en formato CSV.\n",
    "Una vez cargados en memoria, se muestra uno de los registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.538, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, black=396.9, lstat=4.98, medv=24.0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc= SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "house_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train.csv')\n",
    "house_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran las columnas de cada registro para validar que la estructura coincida con la descrita por el proveedor de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- crim: double (nullable = true)\n",
      " |-- zn: double (nullable = true)\n",
      " |-- indus: double (nullable = true)\n",
      " |-- chas: integer (nullable = true)\n",
      " |-- nox: double (nullable = true)\n",
      " |-- rm: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- dis: double (nullable = true)\n",
      " |-- rad: integer (nullable = true)\n",
      " |-- tax: integer (nullable = true)\n",
      " |-- ptratio: double (nullable = true)\n",
      " |-- black: double (nullable = true)\n",
      " |-- lstat: double (nullable = true)\n",
      " |-- medv: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_df.cache()\n",
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se usa el módulo VectorAssembler para preparar los datos en el formato que requiere el algoritmo, que es una matriz con un vector con los valores de las variables de entrada en una columna, y el listado de los valores de salida en la otra columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.538, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, black=396.9, lstat=4.98, medv=24.0, features=DenseVector([1.0, 0.0063, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2, 4.09, 1.0, 296.0, 15.3, 396.9, 4.98]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['ID', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat'], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(house_df)\n",
    "vhouse_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos preparados, se muestran algunos registros para visualizar la estructura con la que quedaron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|medv|\n",
      "+--------------------+----+\n",
      "|[1.0,0.00632,18.0...|24.0|\n",
      "|[2.0,0.02731,0.0,...|21.6|\n",
      "|[4.0,0.03237,0.0,...|33.4|\n",
      "+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vhouse_df = vhouse_df.select(['features', 'medv'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente paso se divide la colección generada previamente de modo que una parte se usará para el entrenamiento y otra más pequeña para la validación posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llega el momento de calcular el modelo, lo cual se hace con la clase LinearRegression. En este caso se realizan 10 iteraciones, y con la función fit se ejecuta el entrenamiento como tal.\n",
    "Luego se muestran los coeficientes obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,-0.026895761647479616,0.0016809042183590526,-0.02088242972506097,4.027246690305449,-0.8491001863067169,4.546433607112968,-0.007751772613229762,-0.5548430910818342,0.0,0.0,-0.5439745978373777,0.011492398377539742,-0.5340979987762313]\n",
      "Intercept: 9.873740121004404\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='medv', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de los coeficientes indican la correlación entre las columnas que representan y los valores de salida, siendo aquellos cercanos a cero los que menos influencia tienen.\n",
    "Ahora se calcula el error medio, que según el tutorial se debe analizar a la par con los valores mínimo y máximo de la variable de salida y su valor promedio, lo cual se hace en el siguiente bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.688934\n",
      "r2: 0.706394\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              medv|\n",
      "+-------+------------------+\n",
      "|  count|               231|\n",
      "|   mean|22.654545454545456|\n",
      "| stddev| 8.672287880086712|\n",
      "|    min|               5.6|\n",
      "|    max|              50.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el modelo calculado se pueden hacer predicciones usando la colección de datos de prueba, y en este ejemplo se muestran sólo cinco registros.\n",
    "El valor de R al cuadrado indica el porcentaje de valores que son representados por el modelo obtenido, que si bien es alto, no incluye a la mayoría. Sería óptimo un valor de 90% o más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+--------------------+\n",
      "|        prediction|medv|            features|\n",
      "+------------------+----+--------------------+\n",
      "|21.911970389472913|20.4|[14.0,0.62976,0.0...|\n",
      "|21.278192127157062|18.2|[15.0,0.63796,0.0...|\n",
      "|22.986537309662076|23.1|[17.0,1.05393,0.0...|\n",
      "|12.757345633273603|12.7|[31.0,1.13081,0.0...|\n",
      "|19.666229494534186|14.5|[32.0,1.35472,0.0...|\n",
      "+------------------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.687931\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"medv\",\"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"medv\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.70307\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se muestra un resumen del proceso de cálculo, donde se indican las iteraciones realizadas, y los residuos calculados para cada punto de los datos de entrenamiento del mejor modelo generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 11\n",
      "objectiveHistory: [0.5000000000000009, 0.43559638992295074, 0.2446733951536692, 0.2189648178145387, 0.18996521723876508, 0.18668164521224045, 0.18544547818184298, 0.18500211685254375, 0.1847540013515694, 0.18469144293499798, 0.1846448822958096]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -6.095562787911838|\n",
      "|-3.5491424123211885|\n",
      "|  3.071528591027473|\n",
      "|  6.510617882814174|\n",
      "|0.24606985442336793|\n",
      "| -4.339900174348621|\n",
      "|-2.8098134644748285|\n",
      "| 1.1411217866334837|\n",
      "|-1.5193145345208805|\n",
      "| 2.9053671197045503|\n",
      "|0.25329291185277825|\n",
      "| 0.4730843494881469|\n",
      "|-2.1601470251526074|\n",
      "|-0.5942806209686768|\n",
      "|-1.6225845145054798|\n",
      "| -1.148034260831734|\n",
      "|  1.957127425808526|\n",
      "| 2.9768314407289154|\n",
      "|-0.4324113729466639|\n",
      "|-2.1563863634632945|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
